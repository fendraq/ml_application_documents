{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "295d2655",
   "metadata": {},
   "source": [
    "# Planering av examination\n",
    "## Frågeställning\n",
    "- Vilka förutsättningar behöver ett dataset ha för att kunna träna en modell att prediktera om en sida är en förstasida eller inte i en text och avgöra var textelementen rubrik, text, bild, tabell och list?\n",
    "- Vilken av följande modeller ger högst score och flest korrekta prediktioner i dessa?\n",
    "\n",
    "## Källor\n",
    "https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html - Exempel på hur man kan arbeta med texter från scikit learn\n",
    "https://www.kaggle.com/datasets/devashishprasad/documnet-layout-recognition-dataset-publaynet-t0/data - Ett dataset med dokument som är annoterade med title, text, figure, table och list\n",
    "https://guillaumejaume.github.io/FUNSD/description/ - Ett dataset med inskannade dokument för dokumentklassificering\n",
    "https://labelformat.com/formats/object-detection/coco/ - Förklaring till COCO dataset format. Även: https://cocodataset.org/#format-data \n",
    "\n",
    "Använt för tips och idéer:\n",
    "https://github.com/AyanGadpal/Document-Image-Augmentation\n",
    "https://github.com/DevashishPrasad/CascadeTabNet\n",
    "\n",
    "## Arbetsordning\n",
    "1. Hantera data så att den går att använda\n",
    "    - PLN:\n",
    "        - Json-filen med annotationerna var väldigt stor så datorn klarade inte av att läsa in den rakt upp och ned istället fick jag skriva ett script som skulle hantera filen genom att läsa in delar med *ijson* och jag använde *polars* istället för pandas. Detta för att lättare kunna spara ned datan med *parquet* (ge lite grund till både polars och parquet). Eftersom 'annotations' var stor fick jag ta den i chunks.\n",
    "        - Det visade sig också att json-filen innehöll samtliga annotationer för hela original-datasetet, men antalet dokument som fanns tillgängligt på kaggle.com innehöll knappt 48 000 dokument. Därför filtrerade jag DataFrame till att innehålla bara de rader som jag hade dokument för.\n",
    "        - Förbereda dataset till binär klassficering av title_page True/False genom att lägga till en kolumn som står för om en bild är title_paage eller inte. Samtliga filer med '00000' är title page. Tog bort alla kolumner förutom title_page och file_name och behöll sedan en rad per unikt filnamn. Sedan checkade jag så att alla filnamn stämde överens mellan DF och /documents.\n",
    "        - Förberedde bilderna genom att sätta gråskala, resize, normalisera pixelvärdena, flatten. Eftersom det var så många bilder körde jag dem i batcher.\n",
    "        - Skapar ett mindre dataset för att utforska hur jag ska bygga en pipeline.\n",
    "        - Utforskar hur RandomForest, SVM, LogisticRegression och KNeighborsClassifier beter sig på det mindre datasetet, genom att sätta upp olika hyperparametrar och testar genom GridSearchCV.\n",
    "        - Utvärderar modellerna med den skapade funktionen evaluate_model() som inkluderar de grundläggande utvärderingmoåtten.\n",
    "        - Hur ska jag tänka eftersom annotationerna är utifrån original pixelstorlek.\n",
    "        - Hur ska jag kunna använda ord som keywords för att hitta relevanta texter? ÄR det lönt eller meningslöst. Bara databassök?\n",
    "        - Plocka bort rad 41-128 för de är inte intressanta för första sidan. Lägga i pipeline för att engineer data innan träning.\n",
    "        - Testar träning på förändrad data på både RFC och KNC samt vilka observationer som de predikterar fel eller rätt för att se om det är intressant att göra en ensemble, ex voting. Sammanfattningsvis ger RFC bra resultat vid threshold 0.2. Några fler FP men få FN. Ensemble med att ta ut de positiva prediktionerna från både KNC och RFC ger dock samma TP men färre FP\n",
    "        - Skapar ett skript för utifrån ensemble för att testa på hela datasetet (dock konverterat till 128x128)\n",
    "    - FUNSD:\n",
    "        - \n",
    "Hur ska jag jobba med bilder?\n",
    "2. Förstå hur det fungerar med dokumentigenkänning, element i dokument, text\n",
    "    Steg att ta:\n",
    "        - Binärt klassificering av dokument: Är det en förstasida eller inte.\n",
    "        - Klustringsproblem?: Vilka förstasidor är av samma kategori?\n",
    "        - Vilka textelement är av samma typ?\n",
    "3. Använda ett färdigt exempel med dataset från Scikit Learn\n",
    "\n",
    "\n",
    "## COCO dataset format\n",
    "Ett standardformat för objekt-detektion och segmentering, som består av tre huvudnycklar: Images, Categories och Annotations. I Images ges detaljer om bilden/dokumentet i form av file_name, height, width och id. Categories ger de olika kategorierna som finns i bilden utifrån id som i sin tur är kopplat till name, ex id: 1, name: \"text\". Det kan också finnas supercategory, alltså överkategori. I Annotations finns följande:\n",
    "- segmentation, som ger en lista med polygon-koordinater, vilka definierar exakta pixelgränser för kategorin. \n",
    "- bbox, i formatet [x, y, width, height] och som ger minsta rektangel som omsluter objektet, mätt från övre vänstra hörnet.\n",
    "- area, som visar segmentets area.\n",
    "- iscrowd i formatet 0 eller 1, som visar om det är ett enskilt objekt eller en grupp av objekt.\n",
    "- image_id, som är en referens till bilden, till vilken annotationen tillhör.\n",
    "- category_id i heltal, som anger vilken kategori (Categories) som segmentet tillhör.\n",
    "- id, som är ett unikt id för respektive annotation.\n",
    "\n",
    "Sammanfattningsvis har en bild många annotationer, varje annotation tillhör en kategori, annotationen har exakta gränser samt en rektangel med areauträkning.\n",
    "\n",
    "## Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f716062a",
   "metadata": {},
   "source": [
    "# Presentation\n",
    "## Ville göra något med text: \n",
    "    - Avgöra om en sida är förstasidan eller inte\n",
    "    - Avgöra var de olika textelementen rubrik, text, bild, tabell och lista\n",
    "\n",
    "## Dataset\n",
    "Pub Lay Net - Är ett av IBM skapat dataset med texter om 4-8 sidor. \n",
    "- 48 000 dokument (originalet innehåller över 300 000) i jpg-format\n",
    "- json med annotationer utifrån COCO med över 3.000.000 rader och annotationer till dokument jag inte hade.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae2c9e7",
   "metadata": {},
   "source": [
    "## Resultat på 128x128\n",
    "\n",
    "### Med PCA\n",
    "```code\n",
    "Setting threshold to 0.5\n",
    "Testing prediction\n",
    "[[8239   38]\n",
    " [  94 1221]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      1.00      0.99      8277\n",
    "           1       0.97      0.93      0.95      1315\n",
    "\n",
    "    accuracy                           0.99      9592\n",
    "   macro avg       0.98      0.96      0.97      9592\n",
    "weighted avg       0.99      0.99      0.99      9592\n",
    "\n",
    "Setting threshold to 0.4\n",
    "Testing prediction\n",
    "[[8183   94]\n",
    " [  65 1250]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      0.99      0.99      8277\n",
    "           1       0.93      0.95      0.94      1315\n",
    "\n",
    "    accuracy                           0.98      9592\n",
    "   macro avg       0.96      0.97      0.97      9592\n",
    "weighted avg       0.98      0.98      0.98      9592\n",
    "```\n",
    "\n",
    "\n",
    "### Utan PCA\n",
    "```code\n",
    "Setting threshold to 0.4\n",
    "Testing prediction\n",
    "[[8218   59]\n",
    " [  42 1273]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      0.99      0.99      8277\n",
    "           1       0.96      0.97      0.96      1315\n",
    "\n",
    "    accuracy                           0.99      9592\n",
    "   macro avg       0.98      0.98      0.98      9592\n",
    "weighted avg       0.99      0.99      0.99      9592\n",
    "\n",
    "Setting threshold to 0.5\n",
    "Testing prediction\n",
    "[[8255   22]\n",
    " [  69 1246]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      1.00      0.99      8277\n",
    "           1       0.98      0.95      0.96      1315\n",
    "\n",
    "    accuracy                           0.99      9592\n",
    "   macro avg       0.99      0.97      0.98      9592\n",
    "weighted avg       0.99      0.99      0.99      9592\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
